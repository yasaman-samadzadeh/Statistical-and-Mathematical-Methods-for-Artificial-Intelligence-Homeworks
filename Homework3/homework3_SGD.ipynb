{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:05:08.899451Z",
     "start_time": "2024-06-16T16:05:07.816817Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:05:08.907021Z",
     "start_time": "2024-06-16T16:05:08.899451Z"
    }
   },
   "outputs": [],
   "source": [
    "def backtracking(f, grad_f, x, X, Y):\n",
    "    \"\"\"\n",
    "    This function is a simple implementation of the backtracking algorithm for\n",
    "    the GD (Gradient Descent) method.\n",
    "    \n",
    "    f: function. The function that we want to optimize.\n",
    "    grad_f: function. The gradient of f(x).\n",
    "    x: ndarray. The actual iterate x_k.\n",
    "    \"\"\"\n",
    "    alpha = 1\n",
    "    c = 0.8\n",
    "    tau = 0.25\n",
    "    \n",
    "    while f(x - alpha * grad_f(x, X, Y), X, Y) > f(x, X, Y) - c * alpha * np.linalg.norm(grad_f(x, X, Y), 2) ** 2:\n",
    "        alpha = tau * alpha\n",
    "        \n",
    "        if alpha < 1e-3:\n",
    "            break\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:05:14.871021Z",
     "start_time": "2024-06-16T16:05:14.856507Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, kmax, tolf, tolx, X, Y, do_backtracking,alpha_value=0.01):\n",
    "    # Initialize lists to store the iterates, function values, gradients, and errors\n",
    "    x = [x0]\n",
    "    f_val = [f(x0, X, Y)]\n",
    "    grads = [grad_f(x0, X, Y)]\n",
    "    err = [np.linalg.norm(grad_f(x0, X, Y))]\n",
    "    \n",
    "    for k in range(kmax):\n",
    "        # Compute next iterate we set the step size to 0.01\n",
    "        if do_backtracking == True:\n",
    "            alpha = backtracking(f, grad_f, x[-1], X, Y)\n",
    "        else:\n",
    "            alpha = alpha_value\n",
    "        x_new = x[-1] - grad_f(x[-1], X, Y) * alpha\n",
    "\n",
    "        # Update the lists\n",
    "        x.append(x_new)\n",
    "        f_val.append(f(x_new, X, Y))\n",
    "        grads.append(grad_f(x_new, X, Y))\n",
    "        err.append(np.linalg.norm(grad_f(x_new, X, Y)))\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad_f(x_new, X, Y)) < tolf * np.linalg.norm(grad_f(x0, X, Y)) or np.linalg.norm(x_new - x[-2]) < tolx:\n",
    "            break\n",
    "\n",
    "    return x, k, f_val, grads, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:27:41.440202Z",
     "start_time": "2024-06-16T16:27:41.421403Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(l, grad_l, w0, data, batch_size, n_epochs,alpha):\n",
    "    # Initialize lists to store the iterates, function values, gradients, and errors\n",
    "    w = [w0]\n",
    "    f_val = []\n",
    "    grads = []\n",
    "    err = []\n",
    "\n",
    "    # Unpack the data\n",
    "    x, y = data\n",
    "\n",
    "    # Compute the number of batches\n",
    "    n_batches = len(y) // batch_size\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        indices = np.arange(len(y))\n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "        # Shuffle the data\n",
    "        #indices = np.random.permutation(len(y))\n",
    "        x = x[:,indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            # Select the data for the current batch\n",
    "            x_batch = x[:,i * batch_size:(i + 1) * batch_size]\n",
    "            y_batch = y[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            # Compute the gradient for the current batch\n",
    "            grad = grad_l(w[-1], x_batch, y_batch)\n",
    "\n",
    "            # Update the iterate\n",
    "            w_new = w[-1] - alpha* grad\n",
    "            w.append(w_new)\n",
    "\n",
    "        # Compute the function value, gradient, and error after each epoch\n",
    "        f_val.append(l(w[-1], x, y))\n",
    "        grad_epoch = grad_l(w[-1], x, y)\n",
    "        grads.append(grad_epoch)\n",
    "        err.append(np.linalg.norm(grad_epoch))\n",
    "\n",
    "    return w, f_val, grads, err\n",
    "\n",
    "#theta_list_sgd, f_vals_sgd, grad_vals_sgd, error_vals_sgd = stochastic_gradient_descent_mle(f_mle, grad_f_mle, np.random.randn(K), (Phi_X_train, Y_train), 64, 10000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:05:22.413178Z",
     "start_time": "2024-06-16T16:05:19.345572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the dataset into X and Y\n",
    "Xt = (data.drop('label', axis=1).values).T\n",
    "Yt = (data['label'].values).T\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:06:32.722253Z",
     "start_time": "2024-06-16T16:06:32.708892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 8423)\n"
     ]
    }
   ],
   "source": [
    "def split_data(X, Y, Ntrain):\n",
    "    d, N = X.shape\n",
    "\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_idx = idx[:Ntrain]\n",
    "    test_idx = idx[Ntrain:]\n",
    "\n",
    "    Xtrain = X[:, train_idx]\n",
    "    Ytrain = Y[train_idx]\n",
    "    \n",
    "    Xtest = X[:, test_idx]\n",
    "    Ytest = Y[test_idx]\n",
    "\n",
    "    return (Xtrain, Ytrain), (Xtest, Ytest)\n",
    "\n",
    "\n",
    "print(X_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:14:38.587577Z",
     "start_time": "2024-06-16T16:14:38.580106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define sigmoid\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "def f(xhat,w):\n",
    "    return sigmoid(xhat.T @ w)\n",
    "\n",
    "# Value of the loss\n",
    "def ell(w, X, Y):\n",
    "    N = len(X)\n",
    "    return (1/(2*N)) * np.linalg.norm(f(X, w) - Y,2)**2\n",
    "\n",
    "#Sum of MSE\n",
    "#def ell(w, X, Y):\n",
    "    #return np.mean((Y - f(X, w))**2)\n",
    "\n",
    "# Value of the gradient\n",
    "def grad_ell(w, X, Y):\n",
    "    # print(w.shape)\n",
    "    # print(X.shape)\n",
    "    # print(Y.shape)\n",
    "    temp = f(X,w)\n",
    "    # print(temp.shape)\n",
    "    return (1/len(Y)) * X @ (f(X, w) * (1 - f(X, w)) * (f(X, w) - Y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:28:02.715897Z",
     "start_time": "2024-06-16T16:28:02.465461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs 5 3\n",
      "Accuracy for GD with train ratio 0.1 is 0.466721222040371\n",
      "Accuracy for SGD with train ratio 0.1 is 0.4660392798690671\n",
      "Difference of weights between GD and SGD is 0.05872645249412016 \n",
      "\n",
      "Accuracy for GD with train ratio 0.8 is 0.454601226993865\n",
      "Accuracy for SGD with train ratio 0.8 is 0.9\n",
      "Difference of weights between GD and SGD is 0.22763479421920524 \n",
      "\n",
      "\n",
      "\n",
      "Inputs 3 4\n",
      "Accuracy for GD with train ratio 0.1 is 0.9891834850283604\n",
      "Accuracy for SGD with train ratio 0.1 is 0.9858857670492019\n",
      "Difference of weights between GD and SGD is 0.11930496526216118 \n",
      "\n",
      "Accuracy for GD with train ratio 0.8 is 0.9905044510385757\n",
      "Accuracy for SGD with train ratio 0.8 is 0.9910979228486647\n",
      "Difference of weights between GD and SGD is 0.19977860665447045 \n",
      "\n",
      "\n",
      "\n",
      "Inputs 1 7\n",
      "Accuracy for GD with train ratio 0.1 is 0.48220618808854104\n",
      "Accuracy for SGD with train ratio 0.1 is 0.5169377522318699\n",
      "Difference of weights between GD and SGD is 0.15512614598126362 \n",
      "\n",
      "Accuracy for GD with train ratio 0.8 is 0.9867914144193726\n",
      "Accuracy for SGD with train ratio 0.8 is 0.9889928453494772\n",
      "Difference of weights between GD and SGD is 0.17915515786447542 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def logistic_regression(input1,input2,X, Y, ell, grad_ell, kmax, tolf, tolx, do_backtracking,alpha_value=0.01, SGD_or_GD=1, epoch=3, batch_size=10,train_ratio=0.8):\n",
    "   #filter the data to only include the inputs the user entered\n",
    "    mask = (Y == int(input1)) | (Y == int(input2))\n",
    "\n",
    "    # Use the mask to extract the corresponding columns from X\n",
    "    X_1 = X[:, mask]\n",
    "    Y_1 = Y[mask]\n",
    "    Y_1[Y_1 == int(input1)] = 0\n",
    "    Y_1[Y_1 == int(input2)] = 1\n",
    "   \n",
    "    # Split the data into training and testing sets\n",
    "    (Xtrain, Ytrain), (Xtest, Ytest) = split_data(X_1, Y_1, int(train_ratio*len(Y_1)))\n",
    "\n",
    "    np.random.seed(0)\n",
    "    w0= np.random.normal(0, 0.01, Xtrain.shape[0])\n",
    "    if SGD_or_GD == 1:\n",
    "        w, k, f_val, grads, err= gradient_descent(ell, grad_ell, w0, kmax, tolf, tolx,Xtrain,Ytrain, do_backtracking,alpha_value)\n",
    "    else:\n",
    "        w, f_val, grads, err=stochastic_gradient_descent(ell, grad_ell, w0, (Xtrain, Ytrain), batch_size, epoch,alpha_value)\n",
    "    \n",
    "    last_w = w[-1]\n",
    "    # print(last_w)\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(Ytest)):\n",
    "        # print(f(Xtest[:,i], last_w), Ytest[i])\n",
    "        if f(Xtest[:,i], last_w)>0.5:\n",
    "            if Ytest[i] == 1:\n",
    "                correct += 1\n",
    "        else:\n",
    "            if Ytest[i] == 0:\n",
    "                correct += 1\n",
    "        \n",
    "        # print(f(Xtest[:,i], last_w), Ytest[i])\n",
    "    \n",
    "    return correct/len(Ytest), last_w\n",
    "\n",
    "acc = logistic_regression(5,3,Xt,Yt, ell, grad_ell,100, 1e-6, 1e-6, True, 0.01, 0, 1, 64,0.5)[0]\n",
    "\n",
    "train_ratios = [0.1,0.8]\n",
    "inputs=np.array([(5,3),(3,4),(1,7)])\n",
    "    \n",
    "for input1, input2 in inputs:\n",
    "    print(\"Inputs\", input1, input2)\n",
    "    for train_ratio in train_ratios:\n",
    "        acc_gd, w_gd = logistic_regression(input1,input2,Xt,Yt, ell, grad_ell,100, 1e-6, 1e-6, False, 0.01, 1, 3, 64,train_ratio)\n",
    "        acc_sgd, w_sgd = logistic_regression(input1,input2,Xt,Yt, ell, grad_ell,100, 1e-6, 1e-6, False, 0.01, 0, 3, 64,train_ratio)\n",
    "        print(\"Accuracy for GD with train ratio\", train_ratio, \"is\", acc_gd)\n",
    "        print(\"Accuracy for SGD with train ratio\", train_ratio, \"is\", acc_sgd)\n",
    "        print(\"Difference of weights between GD and SGD is\", np.linalg.norm(w_gd - w_sgd),\"\\n\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
